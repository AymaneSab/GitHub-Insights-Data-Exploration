{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79a815d8-4148-441f-a0c8-29ba91b86d97",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Github Repositories Scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877abe0b-11e7-48b3-99e2-6612361ff68d",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this project , \n",
    "we gonna use the github standard API to retrieve github repositories informations , in order to study and do some analysis for the devleoppment tred , and know the most used language in the last three years "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3222f05",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import The Necessary Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f21bfe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import csv\n",
    "import logging\n",
    "import requests\n",
    "import time\n",
    "import signal\n",
    "import sys\n",
    "import os.path\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from datetime import datetime, timedelta\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.exceptions import RetryError\n",
    "from urllib3.util.retry import Retry\n",
    "from concurrent.futures import ThreadPoolExecutor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df709f96",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Function To Fetch Github Repositories Using Github API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3be9c-8e23-4ea0-ba06-9635da9a2f40",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Divide date range function is used to divide the given date range into smaller intervals :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f692b564-5496-43c0-9c90-43995018345f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def divide_date_range(start_date, end_date, interval):\n",
    "\n",
    "    intervals = []\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date < end_date:\n",
    "        next_date = min(current_date + timedelta(days=interval), end_date)\n",
    "        intervals.append((current_date, next_date))\n",
    "        current_date = next_date + timedelta(days=1)\n",
    "\n",
    "    return intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21689927",
   "metadata": {},
   "source": [
    "#### Fetch repositories function is used to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b22c78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_repositories(start_date, end_date, token, repositories_limit):\n",
    "\n",
    "    logging.info(\"Fetch Repositories Called \")\n",
    "    base_url = \"https://api.github.com/search/repositories\"\n",
    "    headers = {\"Authorization\": f\"Bearer {token}\"}\n",
    "    per_page = 100\n",
    "    page = 1\n",
    "    repositories = []\n",
    "    repositories_count = 0\n",
    "\n",
    "\n",
    "    # Configure the retry mechanism with exponential backoff\n",
    "    \n",
    "    retries = Retry(total = 5, backoff_factor = 0.2, status_forcelist = [429, 500, 502, 503, 504]) # Code Errors\n",
    "    \n",
    "    session = requests.Session()\n",
    "    session.mount(base_url, HTTPAdapter(max_retries = retries))\n",
    "\n",
    "    while repositories_count < repositories_limit:\n",
    "        params = {\n",
    "            \"q\": f\"created:{start_date.date()}..{end_date.date()}\",\n",
    "            \"sort\": \"stars\",\n",
    "            \"order\": \"desc\",\n",
    "            \"per_page\": per_page,\n",
    "            \"page\": page\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            response = session.get(base_url, headers=headers, params=params)\n",
    "            status = response.raise_for_status()\n",
    "            logging.info(f\"Raised Status  = {status} \")\n",
    "\n",
    "            data = response.json()\n",
    "            items = data.get(\"items\", [])\n",
    "\n",
    "            repositories.extend(items)\n",
    "            repositories_count += len(items)\n",
    "\n",
    "            total_count = min(data.get(\"total_count\", 0), repositories_limit)\n",
    "            logging.info(f\"Processed page {page}/{total_count // per_page + 1} | Retrieved repositories: {repositories_count}\")\n",
    "\n",
    "            if repositories_count >= repositories_limit or end_date <= start_date:\n",
    "                logging.info(f\"We Gonna Stop Now , Byy !!!!!\")\n",
    "\n",
    "                break\n",
    "\n",
    "            page += 1\n",
    "\n",
    "        except requests.exceptions.HTTPError as err:\n",
    "            \n",
    "            logging.error(f\"Failed to retrieve repositories from page {page}: {err}\")\n",
    "            \n",
    "            # Retry the request after a certain interval using exponential backoff\n",
    "            time.sleep(retries.get_backoff_time())\n",
    "\n",
    "        # Check rate limit and wait if necessary after reaching the repositories limit\n",
    "        remaining_requests = int(response.headers.get(\"X-RateLimit-Remaining\", 0))\n",
    "        \n",
    "        if remaining_requests == 0:\n",
    "            reset_time = int(response.headers[\"X-RateLimit-Reset\"])\n",
    "            current_time = time.time()\n",
    "            sleep_time = max(reset_time - current_time, 0) + 1  \n",
    "            logging.info(f\"Rate limit reached. Sleeping for {sleep_time} seconds...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    return repositories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b41628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get repository information:\n",
    "def get_repository_info(repository):\n",
    "    repository_info = {\n",
    "        \"id\": repository[\"id\"],\n",
    "        \"url\": repository[\"url\"],\n",
    "        \"name\": repository[\"name\"],\n",
    "        \"owner\": repository[\"owner\"][\"login\"],\n",
    "        \"ownertype\": repository[\"owner\"][\"type\"],\n",
    "        \"created_at\": repository[\"created_at\"],\n",
    "        \"updated_at\": repository[\"updated_at\"],\n",
    "        \"pushed_at\": repository[\"pushed_at\"],\n",
    "        \"language\": repository[\"language\"],\n",
    "        \"has_issues\": repository[\"has_issues\"],\n",
    "        \"stargazers_count\": repository[\"stargazers_count\"],\n",
    "        \"open_issues_count\": repository[\"open_issues_count\"],\n",
    "        \"description\": repository[\"description\"],\n",
    "        \"archive_url\": repository[\"archive_url\"],\n",
    "        \"forks\": repository[\"forks\"],\n",
    "        \"topics\": repository[\"topics\"],\n",
    "        \"license\": repository[\"license\"],\n",
    "        \"allow_forking\": repository[\"allow_forking\"],\n",
    "        \"contributers_url\": repository[\"contributors_url\"],\n",
    "    }\n",
    "    return repository_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3d9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save repositories to CSV file\n",
    "def save_repositories_to_csv(repositories, filename):\n",
    "    keys = [\n",
    "        \"id\",\n",
    "        \"url\",\n",
    "        \"name\",\n",
    "        \"owner\",\n",
    "        \"ownertype\",\n",
    "        \"created_at\",\n",
    "        \"updated_at\",\n",
    "        \"pushed_at\",\n",
    "        \"language\",\n",
    "        \"has_issues\",\n",
    "        \"stargazers_count\",\n",
    "        \"open_issues_count\",\n",
    "        \"description\",\n",
    "        \"archive_url\",\n",
    "        \"forks\",\n",
    "        \"topics\",\n",
    "        \"license\",\n",
    "        \"allow_forking\",\n",
    "        \"contributers_url\",\n",
    "    ]\n",
    "\n",
    "    with open(filename, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "        writer = csv.DictWriter(file, fieldnames=keys)\n",
    "        writer.writeheader()\n",
    "\n",
    "        for repository in repositories:\n",
    "            repository_info = get_repository_info(repository)\n",
    "            writer.writerow(repository_info)\n",
    "\n",
    "        logging.info(f\"Repositories saved to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a095554-70db-42f3-a969-83a0d47fc3e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Get repositories within date range is used to fetch repositories in a given date range :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71afdd02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_repositories_within_date_range(start_date, end_date, token, filename, interval):\n",
    "        \n",
    "    intervals = divide_date_range(start_date, end_date, interval)\n",
    "    repositories = [] \n",
    "    repositories_limit = 950 # define a repo limit , in order to avoid the github api rate limit : \n",
    "\n",
    "    try:\n",
    "        for idx, (start, end) in enumerate(intervals):\n",
    "            logging.info(f\"Processing interval {idx+1}/{len(intervals)}: {start.date()} to {end.date()} ---------------------------------------------------------Enjoy-----------------------------\")\n",
    "            \n",
    "            interval_repositories = fetch_repositories(start, end, token, repositories_limit)\n",
    "            repositories.extend(interval_repositories)\n",
    "\n",
    "            if len(repositories) >= repositories_limit:\n",
    "                logging.info(f\"Reached the repositories limit {repositories_limit}. Sleeping for 20 seconds --------- !\")\n",
    "                time.sleep(20)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Keyboard interruption detected. Saving progress and exiting --------- !!\")\n",
    "        save_repositories_to_csv(repositories, filename)  # Save progress before exiting\n",
    "\n",
    "    return repositories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef4e210-8c90-4d9e-a25d-fe4c9ef2a479",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Usage Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f58dd8fc-1a86-426a-aeea-9a525d66ee7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Configure logging\n",
    "    logging.basicConfig(\n",
    "        filename='repository_fetch.log',\n",
    "        level=logging.INFO,\n",
    "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "        filemode='w'  # Overwrite the log file each run\n",
    "    )\n",
    "\n",
    "    # Set the start and end dates for the range\n",
    "    start_date = datetime(2020, 1, 1)\n",
    "    end_date = datetime(2022, 5, 1)\n",
    "\n",
    "    token = \"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 \"\n",
    "\n",
    "    # Set the filename for saving the CSV file\n",
    "    filename = \"Raw_Repositories.csv\"\n",
    "\n",
    "    # Set the interval for fetching repositories (in days)\n",
    "    interval = 30\n",
    "\n",
    "    repositories = get_repositories_within_date_range(start_date, end_date, token, filename, interval)\n",
    "\n",
    "    if repositories:\n",
    "        save_repositories_to_csv(repositories, filename)\n",
    "    else:\n",
    "        logging.info(\"No repositories found within the specified date range. Exiting...\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3d2c43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2587dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b10419e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d6e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae2d661",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77e319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe3af8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb8893bf-799f-4091-a722-95d13154707d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get more repositories informations  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7bf70b-b96a-4cde-9f56-68450ba258be",
   "metadata": {},
   "source": [
    "## Used Languages URL "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4f357-e023-487f-984b-a8f35da39e48",
   "metadata": {},
   "source": [
    "#### ThreadPoolExecuter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342ed2ff-1a94-49f2-96b9-e71a45f11819",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = [\n",
    "\"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\" , \n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\",\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" ,\n",
    "\"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\" \n",
    "]\n",
    "\n",
    "def fetch_languages_url(repo_url, token):\n",
    "    headers = {'Authorization': f'token {token}'}\n",
    "    response = requests.get(repo_url, headers=headers)\n",
    "\n",
    "    remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "    reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "\n",
    "    if remaining_requests == 0:\n",
    "        remaining_requests = 0  # Reset remaining_requests to indicate rate limit reached\n",
    "        reset_time = time.time() + 1  # Set reset_time to current time + 1 second to force retry\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        repo_data = response.json()\n",
    "        if 'languages_url' in repo_data:\n",
    "            return repo_data['languages_url'], remaining_requests, reset_time\n",
    "\n",
    "    return None, remaining_requests, reset_time\n",
    "\n",
    "def update_csv_file(csv_file, output_csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        print('Opening the input file...')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['languages_url']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'w', newline='') as file:\n",
    "        print('Opening the output file...')\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        retry_count = 0\n",
    "        token_index = 0\n",
    "        tokens_count = len(tokens)\n",
    "\n",
    "        def process_row(row):\n",
    "            nonlocal retry_count, token_index\n",
    "\n",
    "            repo_url = row['url']\n",
    "            token = tokens[token_index]\n",
    "\n",
    "            while True:\n",
    "                languages_url, remaining_requests, reset_time = fetch_languages_url(repo_url, token)\n",
    "\n",
    "                if languages_url is not None or remaining_requests > 0:\n",
    "                    break\n",
    "\n",
    "                retry_count += 1\n",
    "                if retry_count == tokens_count:\n",
    "                    print('Rate limit exceeded for all tokens. Saving the retrieved info and exiting...')\n",
    "                    return\n",
    "                else:\n",
    "                    token_index = (token_index + 1) % tokens_count\n",
    "                    token = tokens[token_index]\n",
    "                    print(f'Rate limit exceeded. Switching to the next token (Attempt {retry_count + 1})...')\n",
    "                    print(f'Switching to token index: {token_index}')\n",
    "\n",
    "            row['languages_url'] = languages_url\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Log progress\n",
    "            repo_url = row['url']\n",
    "            if languages_url is not None:\n",
    "                print(f'Success: Scraped languages URL for repository: {repo_url}')\n",
    "            else:\n",
    "                print(f'Failure: Failed to scrape languages URL for repository: {repo_url}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute...')\n",
    "                time.sleep(60)  # Sleep for 1 minute after every 950 processed repos\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "            for index, row in enumerate(rows):\n",
    "                executor.submit(process_row, row)\n",
    "\n",
    "        print('All repositories processed successfully.')\n",
    "\n",
    "# Example usage\n",
    "input_csv_file = 'FirstData.csv'\n",
    "output_csv_file = 'SecondData.csv'\n",
    "update_csv_file(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7641cd-9d2e-4553-8ddc-49d55e35895d",
   "metadata": {},
   "source": [
    "#### Iterating Over Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cb10b1-4eb1-4b04-9599-19ed35a65594",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_languages_url(repo_url , token):\n",
    "    \n",
    "    try:\n",
    "        headers = {'Authorization': f'token {token}'}\n",
    "        response = requests.get(repo_url, headers=headers)\n",
    "\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        if remaining_requests < 5:\n",
    "            \n",
    "            print(f'No more remaining_requests  : {remaining_requests} , we gonna change the token ' )\n",
    "            response = requests.get(repo_url, headers=headers) # added yesterday\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        response = requests.get(repo_url, headers=headers)\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            repo_data = response.json()\n",
    "            \n",
    "            if 'languages_url' in repo_data:\n",
    "                return repo_data['languages_url'], remaining_requests\n",
    "            \n",
    "            print(\" The field is not found within the api provided information \")\n",
    "\n",
    "        return None , remaining_requests\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print(\" Keyorad Intteruption !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \")\n",
    "        response = str(input(\"Do you want complete scraping , Y / n \"))\n",
    "        \n",
    "        if response == 'Y' or 'y' or 'yes':\n",
    "            print(\"The Programm Will Continue Scraping ------------> \")\n",
    "            return None, remaining_requests\n",
    "\n",
    "        else:\n",
    "            return None , None\n",
    "            systeme.exit(0)   \n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        print('Connection error occurred. Retrying after 10 minutes...')\n",
    "        time.sleep(200)  # Wait for 3 minutes\n",
    "        \n",
    "        return fetch_languages_url(repo_url, token)\n",
    "\n",
    "\n",
    "def update_csv_file(csv_file, output_csv_file, start_index):\n",
    "    \n",
    "    with open(csv_file, 'r') as file:\n",
    "        print('Opening the input file in write mode  >>>>>>>>>>>>>>>>>>>>>>')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['languages_url']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'a', newline='') as file:\n",
    "        print('Opening the output file i append mode  >>>>>>>>>>>>>>>>>>>>>>')\n",
    "        writer = csv.DictWriter(file, fieldnames = headers)\n",
    "\n",
    "        retry_count = 0\n",
    "        token_index = 0\n",
    "\n",
    "        for index, row in enumerate(rows[start_index:], start = start_index):\n",
    "            repo_url = row['url']\n",
    "            token = tokens[token_index]\n",
    "\n",
    "            languages_url, remaining_requests = fetch_languages_url(repo_url, token)\n",
    "\n",
    "            row['languages_url'] = languages_url\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # handling the rate limite probleme  \n",
    "            \n",
    "            if languages_url is not None:\n",
    "                print(f'Success: Scraped languages URL for repository at index {index}')\n",
    "\n",
    "            else:\n",
    "                print(f'Failure: Failed to scrape languages URL for repository at index {index}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute to avoid day rate limit issues  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                time.sleep(60)\n",
    "                \n",
    "            if remaining_requests < 5:\n",
    "                print(f'Rate limit exceeded. Switching to the next token (Attempt {retry_count + 1})...')\n",
    "                token_index = (token_index + 1) % len(tokens)\n",
    "                print(f'Switching to token index: {token_index}')\n",
    "\n",
    "        else:\n",
    "            print('All repositories processed successfully.')\n",
    "\n",
    "\n",
    "input_csv_file = 'FirstData.csv'\n",
    "output_csv_file = 'SecondData.csv'\n",
    "start_index = int(input(\"Enter the index of the beginning: \"))\n",
    "update_csv_file(input_csv_file, output_csv_file, start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db413a2-aeda-4660-901b-bcf78f077565",
   "metadata": {},
   "source": [
    "## Used Languages Percentage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388f9653-83fd-4a4d-a73e-51b6365db2f7",
   "metadata": {},
   "source": [
    "#### ThreadPoolExceuter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803a2f3-2ecc-483c-9d05-394f91842a39",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_languages(languages_url, token):\n",
    "    try:\n",
    "        headers = {'Authorization': f'token {token}'}\n",
    "        response = requests.get(languages_url, headers=headers)\n",
    "\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "        reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "\n",
    "        if remaining_requests == 0:\n",
    "            sleep_time = max(reset_time - time.time(), 0)\n",
    "            if sleep_time > 0:\n",
    "                print(f'Sleeping for {sleep_time} seconds due to rate limit...')\n",
    "                time.sleep(sleep_time)\n",
    "\n",
    "            response = requests.get(languages_url, headers=headers)\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            languages_data = response.json()\n",
    "            return languages_data, remaining_requests, reset_time\n",
    "\n",
    "        return None, remaining_requests, reset_time\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "\n",
    "# Function to update the CSV file with the used languages and their percentages\n",
    "def update_csv_file(csv_file, output_csv_file):\n",
    "    with open(csv_file, 'r') as file:\n",
    "        print('Opening the input file ------------------ ')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['used_languages']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'w', newline='') as file:\n",
    "        print('Opening the output file ------------------ ')\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        token_index = 0\n",
    "        token = tokens[token_index]\n",
    "        remaining_requests = None\n",
    "        reset_time = None\n",
    "        retry_count = 0\n",
    "\n",
    "        def process_row(index, row):\n",
    "            nonlocal retry_count, token_index, token, remaining_requests, reset_time\n",
    "\n",
    "            languages_url = row['languages_url']\n",
    "\n",
    "            if remaining_requests is not None and remaining_requests == 0:\n",
    "                if retry_count == len(tokens) - 1:\n",
    "                    print('Rate limit exceeded for all tokens. Saving the retrieved info and exiting...')\n",
    "                    return\n",
    "                else:\n",
    "                    token_index = (token_index + 1) % len(tokens)\n",
    "                    token = tokens[token_index]\n",
    "                    print(f'Switching to token {token_index + 1}...')\n",
    "\n",
    "                    retry_count += 1\n",
    "                    print(f'Rate limit reached. Retrying in 2 minutes (Attempt {retry_count})...')\n",
    "                    print('Sleeping for 2 minutes...')\n",
    "                    time.sleep(120)  # Sleep for 2 minutes before retrying\n",
    "                    remaining_requests = None\n",
    "                    reset_time = None\n",
    "\n",
    "            languages_data, remaining_requests, reset_time = fetch_languages(languages_url, token)\n",
    "\n",
    "            if languages_data is not None:\n",
    "                languages_info = []\n",
    "                total_bytes = sum(languages_data.values())\n",
    "                for language, bytes_count in languages_data.items():\n",
    "                    language_percentage = (bytes_count / total_bytes) * 100\n",
    "                    languages_info.append(f'{language}: {language_percentage:.2f}%')\n",
    "\n",
    "                row['used_languages'] = ', '.join(languages_info)\n",
    "                writer.writerow(row)\n",
    "                print(f'Success: Scraped used languages for repository at index {index}')\n",
    "            else:\n",
    "                row['used_languages'] = ''\n",
    "                writer.writerow(row)\n",
    "                print(f'Failure: Failed to scrape used languages for repository at index {index}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute...')\n",
    "                time.sleep(60)  # Sleep for 1 minute after every 950 processed repos\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            for index, row in enumerate(rows):\n",
    "                executor.submit(process_row, index, row)\n",
    "\n",
    "        print('All repositories processed successfully.')\n",
    "\n",
    "\n",
    "input_csv_file = 'SecondCleannedRepositories.csv'\n",
    "output_csv_file = 'GithubRepoData.csv'\n",
    "update_csv_file(input_csv_file, output_csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddafabed-5795-4eb4-afb3-e3f132523285",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Iterating Over Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862a8cc5-85fe-485f-add6-b08f1befc493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_languages(languages_url, token):\n",
    "    try:\n",
    "        headers = {'Authorization': f'token {token}'}\n",
    "        response = requests.get(languages_url, headers=headers)\n",
    "\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        if remaining_requests < 5 :\n",
    "            print(f'No more remaining_requests  : {remaining_requests} , we gonna change the token ' )\n",
    "            response = requests.get(languages_url, headers=headers) # added yesterday\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        response = requests.get(languages_url, headers=headers)\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            languages_data = response.json()\n",
    "            return languages_data, remaining_requests\n",
    "\n",
    "        return None, remaining_requests\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print(\" Keyorad Intteruption !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \")\n",
    "        response = str(input(\"Do you want complete scraping , Y / n \"))\n",
    "        \n",
    "        if response == 'Y' or 'y' or 'yes':\n",
    "            print(\"The Programm Will Continue Scraping ------------> \")\n",
    "            return None, remaining_requests\n",
    "\n",
    "        else:\n",
    "            return None , None\n",
    "            systeme.exit(0)   \n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        print('Connection error occurred. Retrying after 10 minutes...')\n",
    "        time.sleep(200)\n",
    "        \n",
    "        return fetch_languages(languages_url, token)\n",
    "\n",
    "\n",
    "# Function to update the CSV file with the used languages and their percentages : \n",
    "\n",
    "def update_csv_file(csv_file, output_csv_file , start_index):\n",
    "    \n",
    "    with open(csv_file, 'r') as file:\n",
    "        print('Opening the input file ------------------ ')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['used_languages']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'a', newline='') as file:\n",
    "        print('Opening the output file ------------------ ')\n",
    "        writer = csv.DictWriter(file, fieldnames = headers)\n",
    "\n",
    "        retry_count = 0\n",
    "        token_index = 0\n",
    "\n",
    "        for index, row in enumerate(rows[start_index:], start = start_index):\n",
    "            languages_url = row['languages_url']\n",
    "            token = tokens[token_index]\n",
    "\n",
    "            languages_data, remaining_requests = fetch_languages(languages_url, token)\n",
    "            \n",
    "            \n",
    "            if languages_data is not None:\n",
    "                                \n",
    "                languages_info = []\n",
    "                total_bytes = sum(languages_data.values())\n",
    "                for language, bytes_count in languages_data.items():\n",
    "                    language_percentage = (bytes_count / total_bytes) * 100\n",
    "                    languages_info.append(f'{language}: {language_percentage:.2f}%')\n",
    "                    \n",
    "                row['used_languages'] = ', '.join(languages_info)\n",
    "                writer.writerow(row) \n",
    "                \n",
    "                print(f'Success: Scraped Used languages By Percentage for repository at index {index}')\n",
    "\n",
    "                    \n",
    "            else:\n",
    "                print(f'Failure: Failed to scrape used languages for repository at index {index}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute to avoid day rate limit issues  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                time.sleep(60)\n",
    "                \n",
    "            if remaining_requests < 5:\n",
    "                print(f'Rate limit exceeded. Switching to the next token (Attempt {retry_count + 1})...')\n",
    "                token_index = (token_index + 1) % len(tokens)\n",
    "                print(f'Switching to token index: {token_index}')\n",
    "                time.sleep(60)\n",
    "\n",
    "                \n",
    "        else:\n",
    "            print('All repositories processed successfully.')\n",
    "                \n",
    "                \n",
    "                \n",
    "input_csv_file = 'Data2.csv'\n",
    "output_csv_file = 'ThirdData.csv'\n",
    "start_index = int(input(\"Enter the index of the beginning: \"))\n",
    "update_csv_file(input_csv_file, output_csv_file , start_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12debfbb-8aba-4d42-9f95-807ffc71e198",
   "metadata": {},
   "source": [
    "## Get The Top 5 Contributers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16de884a-3297-4ceb-898f-84d7527b7b24",
   "metadata": {},
   "source": [
    "#### Thread Pool Executer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2423c5da-2b44-45b9-8817-9898824156b4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import csv\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "tokens = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_contributors(contributors_url, token):\n",
    "    try:\n",
    "        headers = {'Authorization': f'token {token}'}\n",
    "        response = requests.get(contributors_url, headers=headers)\n",
    "\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "        reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "\n",
    "        if remaining_requests == 0:\n",
    "            retry_after = int(response.headers.get('Retry-After', 1))\n",
    "            print(f'Rate limit reached. Retrying after {retry_after} seconds...')\n",
    "            time.sleep(retry_after)\n",
    "            response = requests.get(contributors_url, headers=headers)\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "\n",
    "        retry_count = 0\n",
    "\n",
    "        while response.status_code == 403:\n",
    "            print(f'Rate limit reached. Retrying after exponential backoff...')\n",
    "            retry_after = 2 ** retry_count  # Exponential backoff\n",
    "            time.sleep(retry_after)\n",
    "            response = requests.get(contributors_url, headers=headers)\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "            reset_time = int(response.headers.get('X-RateLimit-Reset', 0))\n",
    "            retry_count += 1\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            contributors_data = response.json()\n",
    "\n",
    "            # Extract contributor details\n",
    "            contributors = []\n",
    "            for contributor in contributors_data[:5]:\n",
    "                username = contributor['login']\n",
    "                contributions = contributor['contributions']\n",
    "                contributor_type = contributor['type']\n",
    "\n",
    "                contributors.append({\n",
    "                    'username': username,\n",
    "                    'contributions': contributions,\n",
    "                    'type': contributor_type\n",
    "                })\n",
    "\n",
    "            return contributors, remaining_requests, reset_time\n",
    "\n",
    "        return None, remaining_requests, reset_time\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "def update_csv_file(csv_file, output_csv_file):\n",
    "    with open(csv_file, 'r+') as file:\n",
    "        print(f'Opening the input file...')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['top_contributors']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'w+', newline='') as file:\n",
    "        print(f'Creating the output file...')\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "        writer.writeheader()\n",
    "\n",
    "        token_index = 0\n",
    "        token = tokens[token_index]\n",
    "        remaining_requests = None\n",
    "        reset_time = None\n",
    "        retry_count = 0\n",
    "\n",
    "        def process_row(index, row):\n",
    "            nonlocal retry_count, token_index, token, remaining_requests, reset_time\n",
    "\n",
    "            contributors_url = row['contributers_url']\n",
    "            token = tokens[token_index]\n",
    "\n",
    "            contributors_data, remaining_requests, reset_time = fetch_contributors(contributors_url, token)\n",
    "\n",
    "            row['top_contributors'] = ''\n",
    "\n",
    "            if contributors_data is not None:\n",
    "                top_contributors = [f\"{contributor['username']} ({contributor['type']}) - {contributor['contributions']} contributions\" for contributor in contributors_data]\n",
    "                row['top_contributors'] = ', '.join(top_contributors)\n",
    "\n",
    "            writer.writerow(row)\n",
    "\n",
    "            # Log progress\n",
    "            repo_url = row['url']\n",
    "            if contributors_data is not None:\n",
    "                print(f'Success: Scraped top contributors for repository at index {index}: {repo_url}')\n",
    "            else:\n",
    "                print(f'Failure: Failed to scrape top contributors for repository at index {index}: {repo_url}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute...')\n",
    "                time.sleep(60)  # Sleep for 1 minute after every 950 requests\n",
    "\n",
    "            if remaining_requests == 0:\n",
    "                retry_count += 1\n",
    "                if retry_count == len(tokens):\n",
    "                    print('Rate limit exceeded for all tokens. Exiting...')\n",
    "                    return\n",
    "                else:\n",
    "                    token_index = (token_index + 1) % len(tokens)\n",
    "                    print(f'Rate limit exceeded. Switching to the next token (Attempt {retry_count + 1})...')\n",
    "                    print(f'Switching to token index: {token_index}')\n",
    "                    print('Sleeping for 2 minutes...')\n",
    "                    time.sleep(120)  # Sleep for 2 minutes before switching to the next token\n",
    "\n",
    "        with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "            for index, row in enumerate(rows):\n",
    "                executor.submit(process_row, index, row)\n",
    "\n",
    "        print('All repositories processed successfully.')\n",
    "\n",
    "input_csv_file = 'SecondCleannedRepositories.csv'\n",
    "output_csv_file = 'finalData.csv'\n",
    "update_csv_file(input_csv_file, output_csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754d2fd1-f917-405c-9550-1428daa7fef7",
   "metadata": {},
   "source": [
    "#### Iterating Over Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faba52f8-04a1-4345-aef7-aef5d3b8ea3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\n",
    "    \"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\",\n",
    "    \"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\",\n",
    "    \"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\"\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_contributors(contributors_url, token):\n",
    "    \n",
    "    try:\n",
    "        headers = {'Authorization': f'token {token}'}\n",
    "        \n",
    "        response = requests.get(contributors_url, headers=headers)\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        if remaining_requests < 5:\n",
    "            print(f'No more remaining_requests  : {remaining_requests} , we gonna change the token ' )\n",
    "            response = requests.get(contributors_url, headers=headers) # added yesterday\n",
    "            remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "            \n",
    "        response = requests.get(contributors_url, headers=headers) # added yesterday\n",
    "        remaining_requests = int(response.headers.get('X-RateLimit-Remaining', 0))\n",
    "\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            contributors_data = response.json()\n",
    "\n",
    "            # Extract contributor details\n",
    "            contributors = []\n",
    "            for contributor in contributors_data[:5]:\n",
    "                username = contributor['login']\n",
    "                contributions = contributor['contributions']\n",
    "                contributor_type = contributor['type']\n",
    "\n",
    "                contributors.append({\n",
    "                    'username': username,\n",
    "                    'contributions': contributions,\n",
    "                    'type': contributor_type\n",
    "                })\n",
    "\n",
    "            return contributors, remaining_requests\n",
    "\n",
    "        return None, remaining_requests\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        \n",
    "        print(\" Keyorad Intteruption !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! \")\n",
    "        response = str(input(\"Do you want complete scraping , Y / n \"))\n",
    "        \n",
    "        if response == 'Y' or 'y' or 'yes':\n",
    "            print(\"The Programm Will Continue Scraping ------------> \")\n",
    "            return None, remaining_requests\n",
    "\n",
    "        else:\n",
    "            return None , None\n",
    "            systeme.exit(0)   \n",
    "\n",
    "    except requests.exceptions.RequestException:\n",
    "        print('Connection error occurred. Retrying after 10 minutes...')\n",
    "        time.sleep(200)\n",
    "        \n",
    "        return fetch_languages(languages_url, token)\n",
    "\n",
    "\n",
    "def update_csv_file(csv_file, output_csv_file , start_index):\n",
    "    with open(csv_file, 'r+') as file:\n",
    "        print(f'Opening the input file >>>>>>>>>>>>>>>>>')\n",
    "        reader = csv.DictReader(file)\n",
    "        headers = reader.fieldnames + ['top_contributors']\n",
    "        rows = list(reader)\n",
    "\n",
    "    with open(output_csv_file, 'a', newline='') as file:\n",
    "        print(f'Openning the output file <<<<<<<<<<<<<<<')\n",
    "        writer = csv.DictWriter(file, fieldnames=headers)\n",
    "\n",
    "        retry_count = 0\n",
    "        token_index = 0\n",
    "\n",
    "        for index, row in enumerate(rows[start_index:], start = start_index):\n",
    "            contributors_url = row['contributers_url']\n",
    "            token = tokens[token_index]\n",
    "\n",
    "            contributors_data, remaining_requests = fetch_contributors(contributors_url, token)\n",
    "\n",
    "            if contributors_data is not None:\n",
    "                top_contributors = [f\"{contributor['username']} ({contributor['type']}) - {contributor['contributions']} contributions\" for contributor in contributors_data]\n",
    "                row['top_contributors'] = ', '.join(top_contributors)\n",
    "                writer.writerow(row)\n",
    "                print(f'Success: Scraped Used languages By Percentage for repository at index {index}')\n",
    "\n",
    "            else:\n",
    "                row['top_contributors'] = ''\n",
    "                writer.writerow(row)\n",
    "                print(f'Failure: Failed to scrape used languages for repository at index {index}')\n",
    "\n",
    "            if (index + 1) % 950 == 0:\n",
    "                print('Sleeping for 1 minute to avoid day rate limit issues  !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!')\n",
    "                time.sleep(60)\n",
    "                \n",
    "            if remaining_requests < 5:\n",
    "                print(f'Rate limit exceeded. Switching to the next token (Attempt {retry_count + 1})...')\n",
    "                token_index = (token_index + 1) % len(tokens)\n",
    "                print(f'Switching to token index: {token_index}')\n",
    "\n",
    "        else:\n",
    "            print('All repositories processed successfully.')\n",
    "\n",
    "input_csv_file = 'ThirdData.csv'\n",
    "output_csv_file = 'finalData.csv'\n",
    "start_index = int(input(\"Enter the index of the beginning: \"))\n",
    "update_csv_file(input_csv_file, output_csv_file ,start_index )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e3e146-2fa2-4d9d-bf41-048432b25907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vp": {
   "vp_config_version": "1.0.0",
   "vp_menu_width": 273,
   "vp_note_display": false,
   "vp_note_width": 0,
   "vp_position": {
    "width": 278
   },
   "vp_section_display": false,
   "vp_signature": "VisualPython"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
